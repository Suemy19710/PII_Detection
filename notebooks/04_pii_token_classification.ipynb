{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "05ed1838",
      "metadata": {
        "id": "05ed1838"
      },
      "source": [
        "# PII DETECTION: Applied Transformer-based Token Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-19_nf078jm4",
      "metadata": {
        "id": "-19_nf078jm4"
      },
      "source": [
        "### What is NER?\n",
        "NER = Named Entity Recognition\n",
        "It is the NLP task that consists of finding and classifying “entities” (important spans of text) in a sentence.\n",
        "\n",
        "In my project, I'm doing PII-focused NER (Personally Identifiable Information), which is a special kind of NER with many more entity types, for example: B-NAME_STUDENT, B-EMAIL,...\n",
        "\n",
        "Therefore, my model learns to tage every token in a sentence with one of these labels (BIO scheme)\n",
        "\n",
        "### What is BERT transformer model?\n",
        "BERT = Bidirectional Encoder Representations from Transformers\n",
        "It is a revolutionary Transformer model published by Google in 2018 that completely changed NLP.\n",
        "\n",
        "**Pre-training**\n",
        "\n",
        "I used Hugging Face, so it already teach general language understanding for my model.\n",
        "\n",
        "I tried to test different models already pre-trained\n",
        "\n",
        "```\n",
        "bert-base-uncased, distilbert/distilbert-base-uncased, dslim/bert-base-NER\n",
        "```\n",
        "**Fine-tuning**\n",
        "\n",
        "I am doing the fine-tuning step on your PII dataset using AutoModelForTokenClassification + Trainer.\n",
        "\n",
        "Fine-tuning helps teach the model detect and understand PII entities (NAME_STUDENT, EMAIL,...). I take a pre-trained checkpoint -> train models with 15 epoches.\n",
        "\n",
        "### Result\n",
        "\n",
        "Based on my training results - validation metrics from 15 epoches fine-tuning runs, I assume:\n",
        "- With model_checkpoint = \"dslim/bert-base-NER\", it shows moderate F1 growth but plateaus around 0.83.\n",
        "- With model_checkpoint = \"bert-base-uncased\".It achieves higher peak F1 (~0.91) and accuracy (~0.999).\n",
        "\n",
        "Three of my model show the same issue: High validation F1 (0.80–0.91) and accuracy (0.997–0.999), but low testing performance. This could be overfitting signal in NER/PII tasks.\n",
        "\n",
        "###What should I do next step?\n",
        "\n",
        "Last time when I talked with teacher Iman, he told me donot focus on the result now, just following back the NER pipeline and fine-tuning, pre-trained; but I don't know where is the problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5961e292",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-keras in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (2.20.1)\n",
            "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tf-keras) (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.6)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
            "Requirement already satisfied: pillow in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (12.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
            "Requirement already satisfied: namex in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "pcbLBuYrErE0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcbLBuYrErE0",
        "outputId": "ae3f5b03-d8d6-4593-b78e-e9001b09c018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers datasets evaluate seqeval accelerate nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "FhBZ625KIs2M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "FhBZ625KIs2M",
        "outputId": "50e5cfd3-6c36-4be1-ef84-cd502826c29d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "15d737b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15d737b8",
        "outputId": "f88db5f3-8d88-42ea-dac1-e88f7a4857f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\teunm\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from collections import Counter\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback\n",
        "import torch\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e8c706cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "e8c706cd",
        "outputId": "3846baa4-b36f-4504-c567-1b6b85e35c89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In today's modern world, where technology has ...</td>\n",
              "      <td>{'NAME_STUDENT': ['Richard', 'Chang'], 'EMAIL'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In today's modern world, where technology has ...</td>\n",
              "      <td>{'NAME_STUDENT': [], 'EMAIL': ['tamaramorrison...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Janice: A Student with a Unique Identity\\n\\nIn...</td>\n",
              "      <td>{'NAME_STUDENT': ['Janice'], 'EMAIL': ['laura5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Christian is a student who goes by the usernam...</td>\n",
              "      <td>{'NAME_STUDENT': ['Christian'], 'EMAIL': [], '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In today's modern world, where technology has ...</td>\n",
              "      <td>{'NAME_STUDENT': ['Aaron Smith', 'Fischer', 'T...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  \\\n",
              "0  In today's modern world, where technology has ...   \n",
              "1  In today's modern world, where technology has ...   \n",
              "2  Janice: A Student with a Unique Identity\\n\\nIn...   \n",
              "3  Christian is a student who goes by the usernam...   \n",
              "4  In today's modern world, where technology has ...   \n",
              "\n",
              "                                                   1  \n",
              "0  {'NAME_STUDENT': ['Richard', 'Chang'], 'EMAIL'...  \n",
              "1  {'NAME_STUDENT': [], 'EMAIL': ['tamaramorrison...  \n",
              "2  {'NAME_STUDENT': ['Janice'], 'EMAIL': ['laura5...  \n",
              "3  {'NAME_STUDENT': ['Christian'], 'EMAIL': [], '...  \n",
              "4  {'NAME_STUDENT': ['Aaron Smith', 'Fischer', 'T...  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"../data/pii_data.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fc29b03a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "K8hgwPQ7JobM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "K8hgwPQ7JobM",
        "outputId": "a3b35e94-bf5d-45aa-a6f5-325a51b600d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>PII_types</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In today's modern world, where technology has ...</td>\n",
              "      <td>{'NAME_STUDENT': ['Richard', 'Chang'], 'EMAIL'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In today's modern world, where technology has ...</td>\n",
              "      <td>{'NAME_STUDENT': [], 'EMAIL': ['tamaramorrison...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Janice: A Student with a Unique Identity\\n\\nIn...</td>\n",
              "      <td>{'NAME_STUDENT': ['Janice'], 'EMAIL': ['laura5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Christian is a student who goes by the usernam...</td>\n",
              "      <td>{'NAME_STUDENT': ['Christian'], 'EMAIL': [], '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In today's modern world, where technology has ...</td>\n",
              "      <td>{'NAME_STUDENT': ['Aaron Smith', 'Fischer', 'T...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  \\\n",
              "0  In today's modern world, where technology has ...   \n",
              "1  In today's modern world, where technology has ...   \n",
              "2  Janice: A Student with a Unique Identity\\n\\nIn...   \n",
              "3  Christian is a student who goes by the usernam...   \n",
              "4  In today's modern world, where technology has ...   \n",
              "\n",
              "                                           PII_types  \n",
              "0  {'NAME_STUDENT': ['Richard', 'Chang'], 'EMAIL'...  \n",
              "1  {'NAME_STUDENT': [], 'EMAIL': ['tamaramorrison...  \n",
              "2  {'NAME_STUDENT': ['Janice'], 'EMAIL': ['laura5...  \n",
              "3  {'NAME_STUDENT': ['Christian'], 'EMAIL': [], '...  \n",
              "4  {'NAME_STUDENT': ['Aaron Smith', 'Fischer', 'T...  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns = ['Text', 'PII_types']\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "jR73mhdyKMLF",
      "metadata": {
        "id": "jR73mhdyKMLF"
      },
      "outputs": [],
      "source": [
        "# def clean_text(text):\n",
        "#     text = str(text).lower()                    # lowercase all words\n",
        "#     text = re.sub(r'[^a-z\\s]', '', text)        # remove punctuation, numbers, special chars\n",
        "#     words = [w for w in text.split() if w not in stop]  #remove stopwords\n",
        "#     return \" \".join(words)\n",
        "\n",
        "# def clean_text_light(text):\n",
        "#     return str(text).lower()\n",
        "\n",
        "# df['clean_text'] = df['Text'].apply(clean_text)\n",
        "# df['clean_text_light'] = df['Text'].apply(clean_text_light)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64371594",
      "metadata": {
        "id": "64371594"
      },
      "source": [
        "Should removed the clean text cause this is a huge issue. PII recognition depends heavily on casing. When everything is lowercased, I remove almost vital semantic signals the model relies one. This is a known NER problem: Lowercasing decreases token-level F1 by 6-12% on average.\n",
        "\n",
        "First, we need to turn the PII string into real dictionary ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d237f404",
      "metadata": {
        "id": "d237f404"
      },
      "outputs": [],
      "source": [
        "df['PII_types'] = df['PII_types'].apply(lambda x: json.loads(x.replace(\"'\", '\"')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Vod_TBR5J0U8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vod_TBR5J0U8",
        "outputId": "436afa70-1126-4c31-e315-c1fcbdc02b10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(df['PII_types'].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6f37a541",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "6f37a541",
        "outputId": "84b101e6-5053-4442-a4b1-938599f49b49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NAME_STUDENT</th>\n",
              "      <td>2486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PHONE_NUM</th>\n",
              "      <td>2480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>EMAIL</th>\n",
              "      <td>2459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>URL_PERSONAL</th>\n",
              "      <td>2454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID_NUM</th>\n",
              "      <td>2440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>USERNAME</th>\n",
              "      <td>2438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STREET_ADDRESS</th>\n",
              "      <td>2378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Count\n",
              "NAME_STUDENT     2486\n",
              "PHONE_NUM        2480\n",
              "EMAIL            2459\n",
              "URL_PERSONAL     2454\n",
              "ID_NUM           2440\n",
              "USERNAME         2438\n",
              "STREET_ADDRESS   2378"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "PII_counts = Counter([pii_type for pii_dict in df['PII_types'] for pii_type, items in pii_dict.items() if items for _ in items])\n",
        "pd.DataFrame.from_dict(PII_counts, orient='index', columns=['Count']).sort_values(by='Count', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "6f5c3b95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f5c3b95",
        "outputId": "8b5f3566-e8e0-468e-da06-c52c5e72b9cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'NAME_STUDENT': ['Richard', 'Chang'],\n",
              " 'EMAIL': ['gwilliams@yahoo.com'],\n",
              " 'USERNAME': ['brandy38'],\n",
              " 'ID_NUM': ['GB41EJEY19489241157815'],\n",
              " 'PHONE_NUM': ['(259)938-7784x08016'],\n",
              " 'URL_PERSONAL': ['https://twitter.com/john51',\n",
              "  'https://youtube.com/c/sallywalker'],\n",
              " 'STREET_ADDRESS': ['711 Golden Overpass, West Andreaville, OH 44115']}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['PII_types'].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eJ8XC_smVWbr",
      "metadata": {
        "id": "eJ8XC_smVWbr"
      },
      "source": [
        "## Create BIO Tagged Text\n",
        "The BIO / IOB format (short for inside, outside, beginning) is a common tagging format for tagging tokens in a chunking task in computational linguistics (ex. named-entity recognition).\n",
        "\n",
        "| Tag        | Meaning                                      | When to use it                                                                 |\n",
        "|------------|----------------------------------------------|---------------------------------------------------------------------------------|\n",
        "| **B-**     | **Begin** – This token **starts** a new entity | Always the **first** token of an entity<br> Use even if the entity is only 1 token long |\n",
        "| **I-**     | **Inside** – This token is **inside** the same entity | Every token **after** the B- tag that still belongs to the same entity      |\n",
        "| **O**      | **Outside** – This token is **not** part of any PII entity | All tokens that are not part of any named entity                                |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8b15b543",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b15b543",
        "outputId": "95d2fd29-f1c7-4805-f4fa-90a11d63d819"
      },
      "outputs": [],
      "source": [
        "# def create_bio_from_text_and_pii(row):\n",
        "#     text = row[\"Text\"]\n",
        "#     pii_dict = row[\"PII_types\"]\n",
        "\n",
        "#     tokens = text.split()\n",
        "#     labels = [\"O\"] * len(tokens)\n",
        "\n",
        "#     for pii_type, values in pii_dict.items():\n",
        "#         if not values:\n",
        "#             continue\n",
        "#         for pii_text in values:\n",
        "#             if not pii_text.strip():\n",
        "#                 continue\n",
        "\n",
        "#             pii_words = pii_text.split()\n",
        "#             n = len(pii_words)\n",
        "\n",
        "#             for i in range(len(tokens) - n + 1):\n",
        "#                 if tokens[i:i+n] == pii_words:\n",
        "#                     labels[i] = f\"B-{pii_type}\"\n",
        "#                     for j in range(1, n):\n",
        "#                         labels[i+j] = f\"I-{pii_type}\"\n",
        "#                     break\n",
        "\n",
        "#     return pd.Series([tokens, labels])\n",
        "\n",
        "# # I convert the doc-level PII dict into token-level BIO labels\n",
        "# df[['tokens', 'bio_labels']] = df.apply(create_bio_from_text_and_pii, axis=1)\n",
        "# print(\"Example:\")\n",
        "# print(\"Text :\", df['Text'].iloc[0][:200])\n",
        "# print(\"Tokens:\", df['tokens'].iloc[0][:20])\n",
        "# print(\"Labels:\", df['bio_labels'].iloc[0][:30])\n",
        "# print(len(df['bio_labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "sS5U6MxxXqIZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS5U6MxxXqIZ",
        "outputId": "dbdc3d65-a027-4273-f309-a9b0c29f5c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example:\n",
            "Text : In today's modern world, where technology has become an integral part of our lives, it is essential for students like Richard Chang to navigate the digital landscape with ease. Richard, with his email address gwilliams@yahoo.com and username brandy38, is well-equipped to embrace the opportunities th\n",
            "Tokens: ['In', \"today's\", 'modern', 'world,', 'where', 'technology', 'has', 'become', 'an', 'integral', 'part', 'of', 'our', 'lives,', 'it', 'is', 'essential', 'for', 'students', 'like', 'Richard', 'Chang', 'to', 'navigate', 'the', 'digital', 'landscape', 'with', 'ease.', 'Richard,', 'with', 'his', 'email', 'address', 'gwilliams@yahoo.com', 'and', 'username', 'brandy38,', 'is', 'well-equipped', 'to', 'embrace', 'the', 'opportunities', 'that', 'the', 'online', 'realm', 'offers.', 'One', 'of', 'the', 'key', 'aspects', 'of', \"Richard's\", 'digital', 'presence', 'is', 'his']\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'B-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'B-USERNAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'O', 'O', 'O', 'O']\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "TOKEN_REGEX = re.compile(r\"\\S+\")\n",
        "DATE_REGEX = re.compile(\n",
        "    r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\"\n",
        ")\n",
        "\n",
        "def create_bio_from_spans(row):\n",
        "    text = row[\"Text\"]\n",
        "    pii_dict = row[\"PII_types\"]\n",
        "\n",
        "    # PII-safe tokenization (keeps punctuation attached)\n",
        "    tokens = [m.group() for m in TOKEN_REGEX.finditer(text)]\n",
        "    spans = [(m.start(), m.end()) for m in TOKEN_REGEX.finditer(text)]\n",
        "\n",
        "    labels = [\"O\"] * len(tokens)\n",
        "\n",
        "    for pii_type, values in pii_dict.items():\n",
        "        for value in values:\n",
        "            if not value.strip():\n",
        "                continue\n",
        "\n",
        "            for match in re.finditer(re.escape(value), text):\n",
        "                span_start, span_end = match.span()\n",
        "                first = True\n",
        "\n",
        "                for i, (ts, te) in enumerate(spans):\n",
        "                    # Overlap condition (CRITICAL)\n",
        "                    if ts < span_end and te > span_start:\n",
        "                        if labels[i] != \"O\":\n",
        "                            continue\n",
        "                        labels[i] = (\n",
        "                            f\"B-{pii_type}\" if first else f\"I-{pii_type}\"\n",
        "                        )\n",
        "                        first = False\n",
        "            for match in DATE_REGEX.finditer(text):\n",
        "                span_start, span_end = match.span()\n",
        "                first = True\n",
        "                for i, (ts, te) in enumerate(spans):\n",
        "                  if ts < span_end and te > span_start:\n",
        "                    if labels[i] != \"O\":\n",
        "                      continue\n",
        "                    labels[i] = (\n",
        "                        f\"B-DATE\" if first else f\"I-DATE\"\n",
        "                    )\n",
        "                    first = False\n",
        "\n",
        "    return pd.Series([tokens, labels])\n",
        "df[['tokens', 'bio_labels']] = df.apply(create_bio_from_spans, axis=1)\n",
        "print(\"Example:\")\n",
        "print(\"Text :\", df['Text'].iloc[0][:300])\n",
        "print(\"Tokens:\", df['tokens'].iloc[0][:60])\n",
        "print(\"Labels:\", df['bio_labels'].iloc[0][:60])\n",
        "print(len(df['bio_labels']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4bbd8e0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bbd8e0b",
        "outputId": "7940abce-02d7-420e-9bde-edf5292746cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi, → O\n",
            "my → O\n",
            "name → O\n",
            "is → O\n",
            "Minh → B-NAME_STUDENT\n",
            "and → O\n",
            "my → O\n",
            "email → O\n",
            "is → O\n",
            "minh123@gmail.com → B-EMAIL\n",
            "and → O\n",
            "phone → O\n",
            "0908-123-456 → B-PHONE_NUM\n"
          ]
        }
      ],
      "source": [
        "# Test on new sentence\n",
        "test_sentence = \"Hi, my name is Minh and my email is minh123@gmail.com and phone 0908-123-456\"\n",
        "\n",
        "test_tokens = test_sentence.split()\n",
        "test_labels = [\"O\"] * len(test_tokens)\n",
        "\n",
        "# Use the same logic to mark\n",
        "fake_pii = {\n",
        "    \"NAME_STUDENT\": [\"Minh\"],\n",
        "    \"EMAIL\": [\"minh123@gmail.com\"],\n",
        "    \"PHONE_NUM\": [\"0908-123-456\"]\n",
        "}\n",
        "\n",
        "for pii_type, values in fake_pii.items():\n",
        "    for val in values:\n",
        "        words = val.split()\n",
        "        for i in range(len(test_tokens) - len(words) + 1):\n",
        "            if test_tokens[i:i+len(words)] == words:\n",
        "                test_labels[i] = f\"B-{pii_type}\"\n",
        "                for j in range(1, len(words)):\n",
        "                    test_labels[i+j] = f\"I-{pii_type}\"\n",
        "\n",
        "# Show result\n",
        "for token, label in zip(test_tokens, test_labels):\n",
        "    print(token, \"→\", label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ektDDOYdWm9T",
      "metadata": {
        "id": "ektDDOYdWm9T"
      },
      "source": [
        "One of the problem is exact string matching creates memorization, not NER\n",
        "\n",
        "So the model learns:\n",
        "\n",
        "“This string is EMAIL”\n",
        "\n",
        "instead of:\n",
        "\n",
        "“This pattern is EMAIL”\n",
        "\n",
        "That’s why:\n",
        "\n",
        "Validation F1 ≈ 0.9\n",
        "\n",
        "Real inference ≈ fails\n",
        "\n",
        "This is the #1 reason your model still “cannot recognize PII”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oX-SKTIjScEV",
      "metadata": {
        "id": "oX-SKTIjScEV"
      },
      "source": [
        "## Prepare Dataset\n",
        "Currently PII_Types are formed text (EMAIL, PHONE_NUM, etc), I need to collect all PII types to define the label set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "135f02ad",
      "metadata": {
        "id": "135f02ad"
      },
      "outputs": [],
      "source": [
        "# from collections import defaultdict\n",
        "\n",
        "# # Collect only the real entity types\n",
        "# all_labels = set()\n",
        "# for labels in df[\"bio_labels\"]:\n",
        "#     for label in labels:\n",
        "#         if label != \"O\":\n",
        "#             all_labels.add(label)\n",
        "\n",
        "# unique_labels = [\"O\"] + sorted(all_labels)\n",
        "# label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "# id2label = {idx: label for label, idx in label2id.items()}\n",
        "\n",
        "# print(\"Total labels:\", len(unique_labels))\n",
        "# print(label2id)\n",
        "# print(all_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "OnC9VJsdbbWC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnC9VJsdbbWC",
        "outputId": "46813d8d-dcde-45d6-ded0-93bb42b37466"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n",
            "{'O': 0, 'B-NAME_STUDENT': 1, 'I-NAME_STUDENT': 2, 'B-EMAIL': 3, 'I-EMAIL': 4, 'B-PHONE_NUM': 5, 'I-PHONE_NUM': 6, 'B-USERNAME': 7, 'I-USERNAME': 8, 'B-URL_PERSONAL': 9, 'I-URL_PERSONAL': 10, 'B-ID_NUM': 11, 'I-ID_NUM': 12, 'B-STREET_ADDRESS': 13, 'I-STREET_ADDRESS': 14, 'B-DATE': 15, 'I-DATE': 16}\n"
          ]
        }
      ],
      "source": [
        "ENTITY_TYPES = [\n",
        "    \"NAME_STUDENT\",\n",
        "    \"EMAIL\",\n",
        "    \"PHONE_NUM\",\n",
        "    \"USERNAME\",\n",
        "    \"URL_PERSONAL\",\n",
        "    \"ID_NUM\",\n",
        "    \"STREET_ADDRESS\",\n",
        "    \"DATE\"\n",
        "]\n",
        "\n",
        "labels = [\"O\"]\n",
        "for ent in ENTITY_TYPES:\n",
        "    labels.append(f\"B-{ent}\")\n",
        "    labels.append(f\"I-{ent}\")\n",
        "\n",
        "label2id = {l: i for i, l in enumerate(labels)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "print(len(labels))\n",
        "print(label2id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oD9WSr9SWMOR",
      "metadata": {
        "id": "oD9WSr9SWMOR"
      },
      "source": [
        "Convert your DataFrame to a Hugging Face Dataset. Ensure labels are numeric IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "vg99xq_DHDcn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg99xq_DHDcn",
        "outputId": "89cd7469-b0ea-496d-839a-aadb44758b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['tokens', 'bio_labels', 'ner_tags'],\n",
            "        num_rows: 1600\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['tokens', 'bio_labels', 'ner_tags'],\n",
            "        num_rows: 400\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def preprocess_example(example):\n",
        "    example[\"ner_tags\"] = [label2id[label] for label in example[\"bio_labels\"]]\n",
        "    return example\n",
        "\n",
        "# Apply creating the dataset\n",
        "df = df.apply(preprocess_example, axis=1)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "dataset = Dataset.from_pandas(df[['tokens', 'bio_labels', 'ner_tags']])\n",
        "\n",
        "# Split into train/test\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "D5KKAkB5kEE0",
      "metadata": {
        "id": "D5KKAkB5kEE0"
      },
      "outputs": [],
      "source": [
        "# df['bio_labels'].sample(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ikMtxteUWTff",
      "metadata": {
        "id": "ikMtxteUWTff"
      },
      "source": [
        "## Load a Pre-trained Model and Tokenizer\n",
        "Using a model \"distilbert-base-uncased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "af12ae74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af12ae74",
        "outputId": "0148ac3c-1341-40cc-cf1b-2fb4371164b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\teunm\\anaconda3\\envs\\yolo-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\teunm\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "# model_checkpoint = \"dslim/bert-base-NER\"\n",
        "# model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "num_labels = len(label2id)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bDe0wCCgWpKy",
      "metadata": {
        "id": "bDe0wCCgWpKy"
      },
      "source": [
        "## Tokenize and Align Labels\n",
        "Transformers use subword tokenization (e.g., \"minh123@gmail.com\" might split into \"minh\", \"##123\", \"@\", \"gmail\", \".\", \"com\"), so you need to align your word-level labels to subwords. Ignore labels for special tokens like [CLS]/[SEP], and for subwords after the first, use the continuation label (e.g., if word is \"I-EMAIL\", subwords get -100 to ignore in loss, or continue as \"I-EMAIL\" depending on strategy—Hugging Face handles this).\n",
        "Define a tokenization function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "J6X8GABGWuuk",
      "metadata": {
        "id": "J6X8GABGWuuk"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "    all_labels = []\n",
        "\n",
        "    for i, word_labels in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # CLS, SEP, PAD\n",
        "                label_ids.append(-100)\n",
        "\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # First subword of a token\n",
        "                label_ids.append(word_labels[word_idx])\n",
        "\n",
        "            else:\n",
        "                # Continuation subword\n",
        "                label = word_labels[word_idx]\n",
        "                label_name = id2label[label]\n",
        "\n",
        "                if label_name.startswith(\"B-\"):\n",
        "                    # Convert B-XXX → I-XXX\n",
        "                    label_ids.append(label + 1)\n",
        "                else:\n",
        "                    label_ids.append(label)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        all_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = all_labels\n",
        "    return tokenized_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e8f6869d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1600/1600 [00:01<00:00, 1460.41 examples/s]\n",
            "Map: 100%|██████████| 400/400 [00:00<00:00, 1468.67 examples/s]\n"
          ]
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "nlLiWBj4aQpy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlLiWBj4aQpy",
        "outputId": "67f7a761-b8df-4c20-b99d-102e016716c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] -100 IGN\n",
            "in 0 O\n",
            "today 0 O\n",
            "' 0 O\n",
            "s 0 O\n",
            "modern 0 O\n",
            "world 0 O\n",
            ", 0 O\n",
            "where 0 O\n",
            "technology 0 O\n",
            "has 0 O\n",
            "become 0 O\n",
            "an 0 O\n",
            "integral 0 O\n",
            "part 0 O\n",
            "of 0 O\n",
            "our 0 O\n",
            "lives 0 O\n",
            ", 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "not 0 O\n",
            "surprising 0 O\n",
            "to 0 O\n",
            "see 0 O\n",
            "students 0 O\n",
            "like 0 O\n",
            "jesus 1 B-NAME_STUDENT\n",
            "kaufman 2 I-NAME_STUDENT\n",
            "embracing 0 O\n",
            "the 0 O\n",
            "digital 0 O\n",
            "age 0 O\n",
            ". 0 O\n",
            "with 0 O\n",
            "an 0 O\n",
            "email 0 O\n",
            "address 0 O\n",
            "like 0 O\n",
            "daniel 3 B-EMAIL\n",
            "##90 4 I-EMAIL\n",
            "@ 4 I-EMAIL\n",
            "yahoo 4 I-EMAIL\n",
            ". 4 I-EMAIL\n",
            "com 4 I-EMAIL\n",
            ", 4 I-EMAIL\n",
            "jesus 0 O\n",
            "is 0 O\n",
            "ready 0 O\n",
            "to 0 O\n",
            "connect 0 O\n",
            "with 0 O\n",
            "the 0 O\n",
            "world 0 O\n",
            "and 0 O\n",
            "make 0 O\n",
            "the 0 O\n",
            "most 0 O\n",
            "of 0 O\n",
            "the 0 O\n",
            "opportunities 0 O\n",
            "that 0 O\n",
            "come 0 O\n",
            "his 0 O\n",
            "way 0 O\n",
            ". 0 O\n",
            "when 0 O\n",
            "it 0 O\n",
            "comes 0 O\n",
            "to 0 O\n",
            "online 0 O\n",
            "platforms 0 O\n",
            ", 0 O\n",
            "jesus 0 O\n",
            "is 0 O\n",
            "no 0 O\n",
            "stranger 0 O\n",
            ". 0 O\n",
            "with 0 O\n",
            "user 0 O\n",
            "##name 0 O\n",
            "##s 0 O\n",
            "like 0 O\n",
            "rs 7 B-USERNAME\n",
            "##ala 8 I-USERNAME\n",
            "##zar 8 I-USERNAME\n",
            "and 0 O\n",
            "courtney 7 B-USERNAME\n",
            "##sh 8 I-USERNAME\n",
            "##ep 8 I-USERNAME\n",
            "##her 8 I-USERNAME\n",
            "##d 8 I-USERNAME\n",
            ", 8 I-USERNAME\n",
            "he 0 O\n",
            "is 0 O\n",
            "actively 0 O\n",
            "engaged 0 O\n",
            "in 0 O\n",
            "various 0 O\n",
            "online 0 O\n",
            "communities 0 O\n",
            ". 0 O\n",
            "these 0 O\n",
            "user 0 O\n",
            "##name 0 O\n",
            "##s 0 O\n",
            "not 0 O\n",
            "only 0 O\n",
            "reflect 0 O\n",
            "his 0 O\n",
            "interests 0 O\n",
            "but 0 O\n",
            "also 0 O\n",
            "showcase 0 O\n",
            "his 0 O\n",
            "desire 0 O\n",
            "to 0 O\n",
            "connect 0 O\n",
            "with 0 O\n",
            "like 0 O\n",
            "- 0 O\n",
            "minded 0 O\n",
            "individuals 0 O\n",
            "and 0 O\n",
            "share 0 O\n",
            "his 0 O\n",
            "thoughts 0 O\n",
            "and 0 O\n",
            "ideas 0 O\n",
            ". 0 O\n",
            "while 0 O\n",
            "jesus 0 O\n",
            "may 0 O\n",
            "not 0 O\n",
            "have 0 O\n",
            "provided 0 O\n",
            "an 0 O\n",
            "id 0 O\n",
            "number 0 O\n",
            "or 0 O\n",
            "phone 0 O\n",
            "number 0 O\n",
            ", 0 O\n",
            "he 0 O\n",
            "has 0 O\n",
            "left 0 O\n",
            "a 0 O\n",
            "digital 0 O\n",
            "footprint 0 O\n",
            "through 0 O\n",
            "his 0 O\n",
            "personal 0 O\n",
            "ur 0 O\n",
            "##l 0 O\n",
            ". 0 O\n",
            "by 0 O\n",
            "visiting 0 O\n",
            "https 9 B-URL_PERSONAL\n",
            ": 10 I-URL_PERSONAL\n",
            "/ 10 I-URL_PERSONAL\n",
            "/ 10 I-URL_PERSONAL\n",
            "twitter 10 I-URL_PERSONAL\n",
            ". 10 I-URL_PERSONAL\n",
            "com 10 I-URL_PERSONAL\n",
            "/ 10 I-URL_PERSONAL\n",
            "juan 10 I-URL_PERSONAL\n",
            "##14 10 I-URL_PERSONAL\n",
            ", 10 I-URL_PERSONAL\n",
            "one 0 O\n",
            "can 0 O\n",
            "catch 0 O\n",
            "a 0 O\n",
            "glimpse 0 O\n",
            "of 0 O\n",
            "jesus 0 O\n",
            "' 0 O\n",
            "s 0 O\n",
            "online 0 O\n",
            "presence 0 O\n",
            "and 0 O\n",
            "perhaps 0 O\n",
            "even 0 O\n",
            "engage 0 O\n",
            "in 0 O\n",
            "meaningful 0 O\n",
            "conversations 0 O\n",
            "with 0 O\n",
            "him 0 O\n",
            ". 0 O\n",
            "however 0 O\n",
            ", 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "important 0 O\n",
            "to 0 O\n",
            "remember 0 O\n",
            "that 0 O\n",
            "behind 0 O\n",
            "the 0 O\n",
            "digital 0 O\n",
            "persona 0 O\n",
            "lies 0 O\n",
            "a 0 O\n",
            "real 0 O\n",
            "person 0 O\n",
            "with 0 O\n",
            "a 0 O\n",
            "physical 0 O\n",
            "address 0 O\n",
            ". 0 O\n",
            "jesus 0 O\n",
            "' 0 O\n",
            "s 0 O\n",
            "street 0 O\n",
            "address 0 O\n",
            ", 0 O\n",
            "49 13 B-STREET_ADDRESS\n",
            "##2 14 I-STREET_ADDRESS\n",
            "reese 14 I-STREET_ADDRESS\n",
            "valley 14 I-STREET_ADDRESS\n",
            "suite 14 I-STREET_ADDRESS\n",
            "76 14 I-STREET_ADDRESS\n",
            "##1 14 I-STREET_ADDRESS\n",
            ", 14 I-STREET_ADDRESS\n",
            "lake 14 I-STREET_ADDRESS\n",
            "stacy 14 I-STREET_ADDRESS\n",
            ", 14 I-STREET_ADDRESS\n",
            "il 14 I-STREET_ADDRESS\n",
            "07 14 I-STREET_ADDRESS\n",
            "##65 14 I-STREET_ADDRESS\n",
            "##4 14 I-STREET_ADDRESS\n",
            ", 14 I-STREET_ADDRESS\n",
            "serves 0 O\n",
            "as 0 O\n",
            "a 0 O\n",
            "reminder 0 O\n",
            "that 0 O\n",
            "he 0 O\n",
            "is 0 O\n",
            "a 0 O\n",
            "part 0 O\n",
            "of 0 O\n",
            "a 0 O\n",
            "community 0 O\n",
            "and 0 O\n",
            "has 0 O\n",
            "a 0 O\n",
            "place 0 O\n",
            "he 0 O\n",
            "calls 0 O\n",
            "home 0 O\n",
            ". 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "this 0 O\n",
            "connection 0 O\n",
            "to 0 O\n",
            "the 0 O\n",
            "physical 0 O\n",
            "world 0 O\n",
            "that 0 O\n",
            "grounds 0 O\n",
            "him 0 O\n",
            "and 0 O\n",
            "provides 0 O\n",
            "a 0 O\n",
            "sense 0 O\n",
            "of 0 O\n",
            "belonging 0 O\n",
            ". 0 O\n",
            "as 0 O\n",
            "an 0 O\n",
            "essay 0 O\n",
            "writer 0 O\n",
            ", 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "fascinating 0 O\n",
            "to 0 O\n",
            "see 0 O\n",
            "how 0 O\n",
            "students 0 O\n",
            "like 0 O\n",
            "jesus 0 O\n",
            "navigate 0 O\n",
            "the 0 O\n",
            "digital 0 O\n",
            "landscape 0 O\n",
            "while 0 O\n",
            "still 0 O\n",
            "maintaining 0 O\n",
            "a 0 O\n",
            "connection 0 O\n",
            "to 0 O\n",
            "the 0 O\n",
            "real 0 O\n",
            "world 0 O\n",
            ". 0 O\n",
            "the 0 O\n",
            "integration 0 O\n",
            "of 0 O\n",
            "technology 0 O\n",
            "into 0 O\n",
            "our 0 O\n",
            "lives 0 O\n",
            "has 0 O\n",
            "opened 0 O\n",
            "up 0 O\n",
            "new 0 O\n",
            "avenues 0 O\n",
            "for 0 O\n",
            "learning 0 O\n",
            ", 0 O\n",
            "communication 0 O\n",
            ", 0 O\n",
            "and 0 O\n",
            "self 0 O\n",
            "- 0 O\n",
            "expression 0 O\n",
            ". 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "through 0 O\n",
            "platforms 0 O\n",
            "like 0 O\n",
            "email 0 O\n",
            ", 0 O\n",
            "user 0 O\n",
            "##name 0 O\n",
            "##s 0 O\n",
            ", 0 O\n",
            "and 0 O\n",
            "personal 0 O\n",
            "ur 0 O\n",
            "##ls 0 O\n",
            "that 0 O\n",
            "students 0 O\n",
            "like 0 O\n",
            "jesus 0 O\n",
            "can 0 O\n",
            "explore 0 O\n",
            "their 0 O\n",
            "passions 0 O\n",
            ", 0 O\n",
            "connect 0 O\n",
            "with 0 O\n",
            "others 0 O\n",
            ", 0 O\n",
            "and 0 O\n",
            "make 0 O\n",
            "their 0 O\n",
            "mark 0 O\n",
            "on 0 O\n",
            "the 0 O\n",
            "world 0 O\n",
            ". 0 O\n",
            "in 0 O\n",
            "conclusion 0 O\n",
            ", 0 O\n",
            "jesus 1 B-NAME_STUDENT\n",
            "kaufman 2 I-NAME_STUDENT\n",
            "' 2 I-NAME_STUDENT\n",
            "s 2 I-NAME_STUDENT\n",
            "presence 0 O\n",
            "in 0 O\n",
            "the 0 O\n",
            "digital 0 O\n",
            "realm 0 O\n",
            "is 0 O\n",
            "evident 0 O\n",
            "through 0 O\n",
            "his 0 O\n",
            "email 0 O\n",
            "address 0 O\n",
            ", 0 O\n",
            "user 0 O\n",
            "##name 0 O\n",
            "##s 0 O\n",
            ", 0 O\n",
            "and 0 O\n",
            "personal 0 O\n",
            "ur 0 O\n",
            "##l 0 O\n",
            ". 0 O\n",
            "however 0 O\n",
            ", 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "important 0 O\n",
            "to 0 O\n",
            "remember 0 O\n",
            "that 0 O\n",
            "behind 0 O\n",
            "the 0 O\n",
            "digital 0 O\n",
            "facade 0 O\n",
            "lies 0 O\n",
            "a 0 O\n",
            "real 0 O\n",
            "person 0 O\n",
            "with 0 O\n",
            "a 0 O\n",
            "physical 0 O\n",
            "address 0 O\n",
            ". 0 O\n",
            "it 0 O\n",
            "is 0 O\n",
            "this 0 O\n",
            "balance 0 O\n",
            "between 0 O\n",
            "the 0 O\n",
            "virtual 0 O\n",
            "and 0 O\n",
            "physical 0 O\n",
            "worlds 0 O\n",
            "that 0 O\n",
            "allows 0 O\n",
            "students 0 O\n",
            "like 0 O\n",
            "jesus 0 O\n",
            "to 0 O\n",
            "thrive 0 O\n",
            "and 0 O\n",
            "make 0 O\n",
            "a 0 O\n",
            "meaningful 0 O\n",
            "impact 0 O\n",
            ". 0 O\n",
            "[SEP] -100 IGN\n"
          ]
        }
      ],
      "source": [
        "sample = tokenized_dataset[\"train\"][0]\n",
        "\n",
        "for tid, lid in zip(sample[\"input_ids\"], sample[\"labels\"]):\n",
        "    print(tokenizer.convert_ids_to_tokens(tid), lid, id2label.get(lid, \"IGN\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acSOMPSNW5Fr",
      "metadata": {
        "id": "acSOMPSNW5Fr"
      },
      "source": [
        "## Set up training\n",
        "Use the Trainer API for simplicity. It handles optimization, evaluation, etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "d2c37d4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "44ac0618bb414fc597e3b9088e6b30d0",
            "9e458e9c650d4b4d906647232200435a",
            "da48d5b7a2e24874afc860896372992f",
            "7be3d5ccf5c045ed944ee9604f4352e6",
            "7538b61eb43d462fb5336ca408472521",
            "2ca8e560828542628c476adf9d1de22a",
            "508e9285280d482b94ffb7755561d621",
            "91c74e61238c41f6be770a32f37839cc",
            "c0b98d90e7354bc591f2c8936fb13755",
            "d8cb06479f7e412cb267af9644ad5dbd",
            "4c296430cade4e089d76474ba8df5ce7",
            "76fd3ac858044727bba80e90f47240af",
            "326910099564403da4a9f5b95d2c50cc",
            "9b7f4d8358ab47438b2b45b2dec63b7a",
            "684170b586944da8bb3aad90c021abfe",
            "1bb249dfc3c44b949bee3f4aca0093a1",
            "05b5458615a040f18d31c1683b6891a3",
            "29e9530b051444aab9d34799bbd7c627",
            "1dc88495ab654578aa1942a96f3def84",
            "364769d58cdf4b60ba42b12b9f2510d6",
            "3ee3603307cb4cd4a6177929caa4a6db",
            "ada891f24a8d44689ce46a6802b52844"
          ]
        },
        "id": "d2c37d4b",
        "outputId": "36f97feb-715f-48db-968a-aa7050618196"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1600/1600 [00:01<00:00, 1438.77 examples/s]\n",
            "Map: 100%|██████████| 400/400 [00:00<00:00, 1442.94 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1300' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1300/1500 09:14 < 01:25, 2.34 it/s, Epoch 13/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.105600</td>\n",
              "      <td>0.061811</td>\n",
              "      <td>0.691374</td>\n",
              "      <td>0.855749</td>\n",
              "      <td>0.764829</td>\n",
              "      <td>0.979893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.026000</td>\n",
              "      <td>0.021794</td>\n",
              "      <td>0.920653</td>\n",
              "      <td>0.946711</td>\n",
              "      <td>0.933500</td>\n",
              "      <td>0.994845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.017400</td>\n",
              "      <td>0.017542</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.948663</td>\n",
              "      <td>0.943048</td>\n",
              "      <td>0.995566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>0.016812</td>\n",
              "      <td>0.941154</td>\n",
              "      <td>0.945930</td>\n",
              "      <td>0.943536</td>\n",
              "      <td>0.995586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.008600</td>\n",
              "      <td>0.017565</td>\n",
              "      <td>0.943382</td>\n",
              "      <td>0.962717</td>\n",
              "      <td>0.952951</td>\n",
              "      <td>0.995662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.007300</td>\n",
              "      <td>0.016518</td>\n",
              "      <td>0.938939</td>\n",
              "      <td>0.963498</td>\n",
              "      <td>0.951060</td>\n",
              "      <td>0.995914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.018825</td>\n",
              "      <td>0.941755</td>\n",
              "      <td>0.972087</td>\n",
              "      <td>0.956680</td>\n",
              "      <td>0.995692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.017160</td>\n",
              "      <td>0.950383</td>\n",
              "      <td>0.968378</td>\n",
              "      <td>0.959296</td>\n",
              "      <td>0.996282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.003600</td>\n",
              "      <td>0.016588</td>\n",
              "      <td>0.953185</td>\n",
              "      <td>0.969744</td>\n",
              "      <td>0.961393</td>\n",
              "      <td>0.996398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.018214</td>\n",
              "      <td>0.951616</td>\n",
              "      <td>0.971306</td>\n",
              "      <td>0.961360</td>\n",
              "      <td>0.996292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.002900</td>\n",
              "      <td>0.018022</td>\n",
              "      <td>0.959337</td>\n",
              "      <td>0.971696</td>\n",
              "      <td>0.965477</td>\n",
              "      <td>0.996590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.018091</td>\n",
              "      <td>0.959514</td>\n",
              "      <td>0.971501</td>\n",
              "      <td>0.965470</td>\n",
              "      <td>0.996620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.018760</td>\n",
              "      <td>0.954554</td>\n",
              "      <td>0.971696</td>\n",
              "      <td>0.963049</td>\n",
              "      <td>0.996429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1300, training_loss=0.020221707408244794, metrics={'train_runtime': 569.4168, 'train_samples_per_second': 42.148, 'train_steps_per_second': 2.634, 'total_flos': 2718321539481600.0, 'train_loss': 0.020221707408244794, 'epoch': 13.0})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric = load(\"seqeval\")\n",
        "\n",
        "# 1. Use the official data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# 3. Apply the function and remove original columns\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./pii_model\",\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    report_to=\"none\",\n",
        "    dataloader_pin_memory = False\n",
        ")\n",
        "\n",
        "# 4. Now create trainer with the collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "8FqZnP5DoXl_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FqZnP5DoXl_",
        "outputId": "4a1c6050-2fcb-49ab-c901-56e0694c7c6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 1600\n",
              "})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "IbUPOtvEoZOd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbUPOtvEoZOd",
        "outputId": "d676c24d-2b98-4d78-c9ce-09c89511c94b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 400\n",
              "})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L9gjsw18XkB7",
      "metadata": {
        "id": "L9gjsw18XkB7"
      },
      "source": [
        "After training, use the model to predict on new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "1390513b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1390513b",
        "outputId": "dbdb93f0-194b-49eb-983c-61385a9cd912"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity: minh, Label: NAME_STUDENT, Score: 0.62\n",
            "Entity: minh123 @ gmail. com, Label: EMAIL, Score: 0.98\n",
            "Entity: phone, Label: PHONE_NUM, Score: 0.37\n",
            "Entity: 0908 - 123 - 456, Label: PHONE_NUM, Score: 0.94\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\"ner\", model=trainer.model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "test_sentence = \"Hi, my name is Minh and my email is minh123@gmail.com and phone 0908-123-456\"\n",
        "results = ner_pipeline(test_sentence)\n",
        "\n",
        "for entity in results:\n",
        "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "J6JQpMu5K8KT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6JQpMu5K8KT",
        "outputId": "bdc75680-45f0-4ac3-a92c-e255923fc8a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TEXT: My name is John Smith.\n",
            "  Entity: john smith., Label: NAME_STUDENT, Score: 0.43\n",
            "\n",
            "TEXT: You can contact me at john.smith@example.com.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Entity: ., Label: EMAIL, Score: 0.70\n",
            "  Entity: @ example. com., Label: EMAIL, Score: 0.93\n",
            "\n",
            "TEXT: London is beautiful in the summer, but London Brown is my classmate.\n",
            "  Entity: london brown, Label: NAME_STUDENT, Score: 0.67\n",
            "\n",
            "TEXT: My phone number is +1 415 555 2671.\n",
            "  Entity: + 1 415 555 2671., Label: PHONE_NUM, Score: 0.86\n",
            "\n",
            "TEXT: I was born on 02/25/2005.\n",
            "  Entity: / 25, Label: PHONE_NUM, Score: 0.19\n",
            "  Entity: /, Label: URL_PERSONAL, Score: 0.18\n",
            "\n",
            "TEXT: I live at 123A Main Street, New York City.\n",
            "  Entity: 123a main street,, Label: STREET_ADDRESS, Score: 0.61\n",
            "  Entity: city, Label: STREET_ADDRESS, Score: 0.30\n",
            "\n",
            "TEXT: Call me at 415-822-4459 tomorrow.\n",
            "  Entity: 415 - 822 - 4459 tomorrow, Label: PHONE_NUM, Score: 0.87\n",
            "\n",
            "TEXT: I'm Emily Johnson, born on 02/25/2005, living at 456 Oak Avenue, Los Angeles.\n",
            "  Entity: ' m, Label: NAME_STUDENT, Score: 0.41\n",
            "  Entity: emily johnson,, Label: NAME_STUDENT, Score: 0.72\n",
            "  Entity: 02, Label: ID_NUM, Score: 0.16\n",
            "  Entity: / 25 /, Label: PHONE_NUM, Score: 0.29\n",
            "  Entity: ,, Label: NAME_STUDENT, Score: 0.18\n",
            "  Entity: 456 oak avenue, los angeles, Label: STREET_ADDRESS, Score: 0.77\n",
            "  Entity: ., Label: NAME_STUDENT, Score: 0.20\n",
            "\n",
            "TEXT: My advisor is Michael Andrew Thompson.\n",
            "  Entity: michael andrew thompson., Label: NAME_STUDENT, Score: 0.35\n",
            "\n",
            "TEXT: The student Sarah O'Connor submitted the form.\n",
            "  Entity: o ' connor, Label: NAME_STUDENT, Score: 0.46\n",
            "\n",
            "TEXT: Yesterday I met Jean-Luc Picard - a science student in class.\n",
            "  Entity: ##ard, Label: NAME_STUDENT, Score: 0.34\n",
            "\n",
            "TEXT: Send the file to alice.bob+test@cs.stanford.edu.\n",
            "  Entity: @ cs. stanford. edu, Label: EMAIL, Score: 0.69\n",
            "\n",
            "TEXT: Her email is user_2024@mail.co.uk.\n",
            "  Entity: @, Label: EMAIL, Score: 0.47\n",
            "  Entity: ., Label: EMAIL, Score: 0.58\n",
            "  Entity: ., Label: EMAIL, Score: 0.44\n",
            "\n",
            "TEXT: Contact: support-team@company.io\n",
            "No PII detected.\n",
            "\n",
            "TEXT: Call me at (212) 555-0198.\n",
            "  Entity: at, Label: PHONE_NUM, Score: 0.31\n",
            "  Entity: ( 212 ) 555 - 0198., Label: PHONE_NUM, Score: 0.98\n",
            "\n",
            "TEXT: My backup number is +44 20 7946 0958.\n",
            "  Entity: + 44 20 7946 0958., Label: ID_NUM, Score: 0.84\n",
            "\n",
            "TEXT: Emergency contact: 0908 123 456.\n",
            "  Entity: 0908 123 456., Label: PHONE_NUM, Score: 0.54\n",
            "\n",
            "TEXT: I live at 1600 Pennsylvania Avenue NW.\n",
            "  Entity: 1600 pennsylvania avenue nw., Label: STREET_ADDRESS, Score: 0.65\n",
            "\n",
            "TEXT: Office location: 221B Baker Street, London.\n",
            "  Entity: 221b, Label: STREET_ADDRESS, Score: 0.35\n",
            "  Entity: ,, Label: STREET_ADDRESS, Score: 0.44\n",
            "\n",
            "TEXT: Ship to 742 Evergreen Terrace.\n",
            "  Entity: 742 evergreen terrace, Label: STREET_ADDRESS, Score: 0.69\n",
            "\n",
            "TEXT: Visit my site at https://johnsmith.dev\n",
            "  Entity: https : / / johnsmith. dev, Label: URL_PERSONAL, Score: 0.95\n",
            "\n",
            "TEXT: My LinkedIn is linkedin.com/in/emilyjohnson\n",
            "  Entity: linked, Label: URL_PERSONAL, Score: 0.28\n",
            "  Entity: . com / in / emilyjohnson, Label: URL_PERSONAL, Score: 0.97\n",
            "\n",
            "TEXT: Portfolio: www.alexchen.me\n",
            "No PII detected.\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load NER pipeline\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=trainer.model,\n",
        "    tokenizer=tokenizer,\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "test_sentences = [\n",
        "    \"My name is John Smith.\",\n",
        "    \"You can contact me at john.smith@example.com.\",\n",
        "    \"London is beautiful in the summer, but London Brown is my classmate.\",\n",
        "    \"My phone number is +1 415 555 2671.\",\n",
        "    \"I was born on 02/25/2005.\",\n",
        "    \"I live at 123A Main Street, New York City.\",\n",
        "    \"Call me at 415-822-4459 tomorrow.\",\n",
        "    \"I'm Emily Johnson, born on 02/25/2005, living at 456 Oak Avenue, Los Angeles.\",\n",
        "    \"My advisor is Michael Andrew Thompson.\",\n",
        "    \"The student Sarah O'Connor submitted the form.\",\n",
        "    \"Yesterday I met Jean-Luc Picard - a science student in class.\",\n",
        "    \"Send the file to alice.bob+test@cs.stanford.edu.\",\n",
        "    \"Her email is user_2024@mail.co.uk.\",\n",
        "    \"Contact: support-team@company.io\",\n",
        "    \"Call me at (212) 555-0198.\",\n",
        "    \"My backup number is +44 20 7946 0958.\",\n",
        "    \"Emergency contact: 0908 123 456.\",\n",
        "\n",
        "    \"I live at 1600 Pennsylvania Avenue NW.\",\n",
        "    \"Office location: 221B Baker Street, London.\",\n",
        "    \"Ship to 742 Evergreen Terrace.\",\n",
        "\n",
        "    \"Visit my site at https://johnsmith.dev\",\n",
        "    \"My LinkedIn is linkedin.com/in/emilyjohnson\",\n",
        "    \"Portfolio: www.alexchen.me\",\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "for text in test_sentences:\n",
        "    print(f\"\\nTEXT: {text}\")\n",
        "    results = ner_pipeline(text)\n",
        "    if not results:\n",
        "        print(\"No PII detected.\")\n",
        "    else:\n",
        "        for entity in results:\n",
        "            print(f\"  Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "qPrIFZ51LvwG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPrIFZ51LvwG",
        "outputId": "42b4875e-d063-4597-c86d-d5755dc79008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to: ./ner_model_2026-01-12\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Create folder name with today's date\n",
        "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "save_dir = f\"./ner_model_{today}\"\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save model and tokenizer\n",
        "trainer.model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "print(f\"Model saved to: {save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4mHM2fEPu4Ql",
      "metadata": {
        "id": "4mHM2fEPu4Ql"
      },
      "outputs": [],
      "source": [
        "# from datetime import datetime\n",
        "# import os\n",
        "\n",
        "# today = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# save_dir = rf\"C:\\Users\\nguye\\OneDrive\\Desktop\\uni\\Y2_Sem3\\individual_project\\PII_Detection_3\\PII_Detection\\notebooks\\checkpoints\\ner_model_{today}\"\n",
        "\n",
        "# os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# trainer.model.save_pretrained(save_dir)\n",
        "# tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# print(f\"Model saved to: {save_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6WMVfSlRLycR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WMVfSlRLycR",
        "outputId": "36517980-3661-4f6d-94a9-da9507adb076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity: minh, Label: NAME_STUDENT, Score: 0.62\n",
            "Entity: minh123 @ gmail. com, Label: EMAIL, Score: 0.98\n",
            "Entity: phone, Label: PHONE_NUM, Score: 0.37\n",
            "Entity: 0908 - 123 - 456, Label: PHONE_NUM, Score: 0.94\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"./ner_model_2026-01-12\",\n",
        "    tokenizer=\"./ner_model_2026-01-12\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "test_sentence = \"Hi, my name is Minh and my email is minh123@gmail.com and phone 0908-123-456\"\n",
        "results = ner_pipeline(test_sentence)\n",
        "\n",
        "for entity in results:\n",
        "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "yolo-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05b5458615a040f18d31c1683b6891a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bb249dfc3c44b949bee3f4aca0093a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc88495ab654578aa1942a96f3def84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e9530b051444aab9d34799bbd7c627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ca8e560828542628c476adf9d1de22a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "326910099564403da4a9f5b95d2c50cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b5458615a040f18d31c1683b6891a3",
            "placeholder": "​",
            "style": "IPY_MODEL_29e9530b051444aab9d34799bbd7c627",
            "value": "Map: 100%"
          }
        },
        "364769d58cdf4b60ba42b12b9f2510d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ee3603307cb4cd4a6177929caa4a6db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ac0618bb414fc597e3b9088e6b30d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e458e9c650d4b4d906647232200435a",
              "IPY_MODEL_da48d5b7a2e24874afc860896372992f",
              "IPY_MODEL_7be3d5ccf5c045ed944ee9604f4352e6"
            ],
            "layout": "IPY_MODEL_7538b61eb43d462fb5336ca408472521"
          }
        },
        "4c296430cade4e089d76474ba8df5ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "508e9285280d482b94ffb7755561d621": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "684170b586944da8bb3aad90c021abfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ee3603307cb4cd4a6177929caa4a6db",
            "placeholder": "​",
            "style": "IPY_MODEL_ada891f24a8d44689ce46a6802b52844",
            "value": " 400/400 [00:03&lt;00:00, 133.19 examples/s]"
          }
        },
        "7538b61eb43d462fb5336ca408472521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76fd3ac858044727bba80e90f47240af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_326910099564403da4a9f5b95d2c50cc",
              "IPY_MODEL_9b7f4d8358ab47438b2b45b2dec63b7a",
              "IPY_MODEL_684170b586944da8bb3aad90c021abfe"
            ],
            "layout": "IPY_MODEL_1bb249dfc3c44b949bee3f4aca0093a1"
          }
        },
        "7be3d5ccf5c045ed944ee9604f4352e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8cb06479f7e412cb267af9644ad5dbd",
            "placeholder": "​",
            "style": "IPY_MODEL_4c296430cade4e089d76474ba8df5ce7",
            "value": " 1600/1600 [00:15&lt;00:00, 101.90 examples/s]"
          }
        },
        "91c74e61238c41f6be770a32f37839cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b7f4d8358ab47438b2b45b2dec63b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc88495ab654578aa1942a96f3def84",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_364769d58cdf4b60ba42b12b9f2510d6",
            "value": 400
          }
        },
        "9e458e9c650d4b4d906647232200435a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ca8e560828542628c476adf9d1de22a",
            "placeholder": "​",
            "style": "IPY_MODEL_508e9285280d482b94ffb7755561d621",
            "value": "Map: 100%"
          }
        },
        "ada891f24a8d44689ce46a6802b52844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0b98d90e7354bc591f2c8936fb13755": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8cb06479f7e412cb267af9644ad5dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da48d5b7a2e24874afc860896372992f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c74e61238c41f6be770a32f37839cc",
            "max": 1600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0b98d90e7354bc591f2c8936fb13755",
            "value": 1600
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
